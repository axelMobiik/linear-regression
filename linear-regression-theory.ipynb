{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0684f16a-167f-434f-bba5-dda9fde83fbe",
   "metadata": {},
   "source": [
    "<h1>Introduction to Linear Regression</h1>\n",
    "<p>\n",
    "    This all started in the 1800's with a guy named Francis Galton. Galton was studying the relationship between parents and their children.\n",
    "    <br>In the particular, he investigated the relationshop between the heights of fathers and their sons.\n",
    "    <br>What he descovered was that a man's son tended to be roughly as tall as his father.\n",
    "    <br>However Galton's beakthrough was that the son's height <strong>tended to be closer to the overall average</strong> height of all people.\n",
    "</p\n",
    "\n",
    "\n",
    "<p>\n",
    "    Let's take Shaquille O'Neal as an example. Shaq is really tall: 7ft 1in (2.2 meters).\n",
    "    <br>If Shaq has a son, chances are he'll be pretty tall too. However, Shaq is such an anomaly that there is also a very good chance that his son will be <strong>not be as tall as Shaq</strong>\n",
    "    <br>Turns out this is the case: Shaq's son is pretty tall (6 ft 7 in), but not nearly as tall as his dad.\n",
    "    <br>Galton called this phenomenon <strong>regression</strong>, as in \"A father's son's height tends to regress (or drift towards) the mean (average) height\".\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "    All we're trying to do when we calculate our regression line is draw a line that's as close to every dot as possible.\n",
    "    <br>For classic linear regression, or \"Least Squares Method\", you only measure the closeness in the \"up and down direction\".\n",
    "    <br>\n",
    "    <br>Now wouldn't it be great if we could apply this same concept to a graph with more than just two data points?\n",
    "    <br>By doing this, we could take multiple men and their son's heights and do things like tell a man how tall we expect his son to be... before he even has a son!\n",
    "    <br>\n",
    "    <br>Out goal with linear regression is to <strong>minimize the vertical distance</strong> between all the data points and our line.\n",
    "    <br>So in determining the <strong>best line</strong>, we are attempting to minimize the distance between <strong>all</strong> the points and their distance to our line\n",
    "    <br>There are lots of different ways to minimize this, (sum of squared errors, sum of absolute errors, etc), but all these methods have a general goal of minimizing this distance.\n",
    "    <br>\n",
    "    <br>For example, one of the most popular methods is the least squares method.\n",
    "    <br>The question is, how do we decide which line is the best fitting one?\n",
    "    <br>We'll use the Least Squares Method, which is fitted by minimizing the <strong>sum of squares of the residuals</strong>.\n",
    "    <br>The residuals for an observation is the diference between the observation (the y-value) and the fitted line.\n",
    "</p></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c4cb09-7e1c-48e1-bee9-df7a2beb24c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
